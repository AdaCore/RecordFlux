from __future__ import annotations

from pathlib import Path
from typing import Final

import pytest
from lsprotocol.types import (
    CodeLens,
    CodeLensParams,
    Command,
    DefinitionParams,
    Location,
    Position,
    Range,
    SemanticTokensParams,
    TextDocumentIdentifier,
    WorkspaceFolder,
)
from pygls.workspace import Workspace
from rflx_ls import model, server

VALID_LSP_TOKEN_CATEGORIES: Final = {
    "namespace",
    "type",
    "class",
    "enum",
    "interface",
    "struct",
    "typeParameter",
    "parameter",
    "variable",
    "property",
    "enumMember",
    "event",
    "function",
    "method",
    "macro",
    "keyword",
    "modifier",
    "comment",
    "string",
    "number",
    "regexp",
    "operator",
    "decorator",
}


@pytest.fixture
def language_server() -> server.RecordFluxLanguageServer:
    language_server = server.RecordFluxLanguageServer("recordflux-ls", "v0.1")
    language_server.lsp.workspace = Workspace(
        Path("tests/data").absolute().as_uri(),
        None,
        workspace_folders=[WorkspaceFolder(Path("tests/data").absolute().as_uri(), "data")],
    )  # type: ignore

    return language_server


def test_lsp_tokens_categories() -> None:
    assert all(
        token_type in VALID_LSP_TOKEN_CATEGORIES
        for token_type in server.LSP_TOKEN_CATEGORIES.keys()
    )

    seen: set[int] = set()

    for token_value in server.LSP_TOKEN_CATEGORIES.keys():
        assert server.LSP_TOKEN_CATEGORIES[token_value] not in seen
        seen.add(server.LSP_TOKEN_CATEGORIES[token_value])


def test_rflx_type_to_lsp_token() -> None:
    assert all(
        server.rflx_type_to_lsp_token(token_category) in server.LSP_TOKEN_CATEGORIES.keys()
        for token_category in model.SymbolCategory
        if not token_category == model.SymbolCategory.UNDEFINED
    )
    assert set(server.LSP_TOKEN_CATEGORIES.keys()) == set(
        [
            server.rflx_type_to_lsp_token(token_category)
            for token_category in model.SymbolCategory
            if not token_category == model.SymbolCategory.UNDEFINED
        ]
    )


@pytest.mark.asyncio
async def test_goto_definition(language_server: server.RecordFluxLanguageServer) -> None:
    positions: dict[tuple[int, int], Location] = {
        (19, 41): Location(
            Path("tests/data/universal.rflx").absolute().as_uri(),
            Range(Position(17, 8), Position(17, 88)),
        )
    }

    for i, position in enumerate(positions):
        params = DefinitionParams(
            TextDocumentIdentifier(Path("tests/data/universal.rflx").absolute().as_uri()),
            Position(position[0], position[1] + (i % 2)),
        )
        assert [positions[position]] == await server.goto_definition(language_server, params)


@pytest.mark.asyncio
async def test_code_lenses(language_server: server.RecordFluxLanguageServer) -> None:
    assert await server.code_lens(
        language_server,
        CodeLensParams(
            TextDocumentIdentifier(Path("tests/data/messages.rflx").absolute().as_uri())
        ),
    ) == [
        CodeLens(
            range=Range(Position(18, 8), Position(22, 17)),
            command=Command(title="Show message graph", command="showMessageGraph", arguments=[15]),
            data=None,
        ),
        CodeLens(
            range=Range(Position(24, 8), Position(30, 43)),
            command=Command(title="Show message graph", command="showMessageGraph", arguments=[16]),
            data=None,
        ),
        CodeLens(
            range=Range(Position(32, 8), Position(37, 43)),
            command=Command(title="Show message graph", command="showMessageGraph", arguments=[17]),
            data=None,
        ),
    ]


@pytest.mark.asyncio
async def test_message_graph(
    language_server: server.RecordFluxLanguageServer,
    monkeypatch: pytest.MonkeyPatch,
    tmp_path: Path,
) -> None:
    monkeypatch.setattr(server, "CACHE_PATH", tmp_path)

    language_server.update_model()
    await server.display_message_graph(language_server, [15])

    assert (server.CACHE_PATH / "graphs" / "Msg.svg").is_file()


@pytest.mark.asyncio
async def test_semantic_tokens(language_server: server.RecordFluxLanguageServer) -> None:
    params = SemanticTokensParams(
        TextDocumentIdentifier(Path("tests/data/universal.rflx").absolute().as_uri())
    )
    tokens = await server.semantic_tokens(language_server, params)

    assert tokens.data == [
        0,
        8,
        9,
        7,
        0,
        2,
        8,
        12,
        1,
        0,
        0,
        17,
        7,
        2,
        0,
        1,
        25,
        7,
        2,
        0,
        1,
        25,
        8,
        2,
        0,
        1,
        25,
        9,
        2,
        0,
        1,
        25,
        15,
        2,
        0,
        1,
        25,
        10,
        2,
        0,
        1,
        25,
        21,
        2,
        0,
        1,
        25,
        24,
        2,
        0,
        2,
        8,
        6,
        0,
        0,
        2,
        8,
        5,
        0,
        0,
        2,
        8,
        6,
        0,
        0,
        0,
        22,
        5,
        0,
        0,
        2,
        8,
        11,
        1,
        0,
        0,
        16,
        7,
        2,
        0,
        0,
        14,
        7,
        2,
        0,
        0,
        46,
        4,
        2,
        0,
        2,
        8,
        12,
        0,
        0,
        0,
        28,
        11,
        1,
        0,
        2,
        8,
        6,
        3,
        0,
        2,
        9,
        11,
        4,
        0,
        0,
        14,
        11,
        1,
        0,
        2,
        18,
        11,
        4,
        0,
        0,
        14,
        7,
        2,
        0,
        1,
        17,
        6,
        4,
        0,
        1,
        18,
        11,
        4,
        0,
        0,
        14,
        7,
        2,
        0,
        -4,
        23,
        11,
        4,
        0,
        5,
        9,
        6,
        4,
        0,
        0,
        9,
        6,
        0,
        0,
        1,
        17,
        4,
        4,
        0,
        1,
        28,
        6,
        4,
        0,
        -2,
        18,
        6,
        4,
        0,
        3,
        9,
        4,
        4,
        0,
        3,
        8,
        7,
        0,
        0,
        0,
        23,
        6,
        3,
        0,
        2,
        8,
        7,
        3,
        0,
        2,
        9,
        12,
        4,
        0,
        0,
        15,
        12,
        1,
        0,
        2,
        18,
        12,
        4,
        0,
        0,
        15,
        7,
        2,
        0,
        1,
        17,
        4,
        4,
        0,
        1,
        28,
        7,
        3,
        0,
        0,
        15,
        12,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        15,
        21,
        2,
        0,
        1,
        17,
        6,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        16,
        7,
        2,
        0,
        1,
        22,
        12,
        4,
        0,
        0,
        16,
        21,
        2,
        0,
        1,
        22,
        12,
        4,
        0,
        0,
        16,
        24,
        2,
        0,
        1,
        17,
        7,
        4,
        0,
        1,
        28,
        7,
        3,
        0,
        0,
        15,
        12,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        15,
        24,
        2,
        0,
        -12,
        24,
        12,
        4,
        0,
        13,
        9,
        6,
        4,
        0,
        0,
        9,
        6,
        0,
        0,
        1,
        17,
        4,
        4,
        0,
        1,
        28,
        6,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        15,
        7,
        2,
        0,
        1,
        17,
        12,
        4,
        0,
        1,
        28,
        6,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        15,
        15,
        2,
        0,
        1,
        17,
        7,
        4,
        0,
        1,
        28,
        6,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        15,
        10,
        2,
        0,
        1,
        17,
        5,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        15,
        8,
        2,
        0,
        1,
        22,
        6,
        4,
        0,
        0,
        9,
        9,
        7,
        0,
        0,
        11,
        5,
        0,
        0,
        1,
        17,
        6,
        4,
        0,
        1,
        28,
        6,
        4,
        0,
        1,
        18,
        12,
        4,
        0,
        0,
        15,
        9,
        2,
        0,
        -15,
        18,
        6,
        4,
        0,
        16,
        9,
        4,
        4,
        0,
        2,
        9,
        12,
        4,
        0,
        0,
        15,
        12,
        0,
        0,
        0,
        0,
        12,
        4,
        0,
        2,
        9,
        7,
        4,
        0,
        0,
        10,
        7,
        0,
        0,
        0,
        0,
        7,
        4,
        0,
        2,
        9,
        5,
        4,
        0,
        0,
        8,
        5,
        0,
        0,
        0,
        0,
        5,
        4,
        0,
        2,
        9,
        6,
        4,
        0,
        0,
        9,
        6,
        0,
        0,
        0,
        0,
        6,
        4,
        0,
        3,
        7,
        7,
        3,
        0,
        0,
        13,
        4,
        4,
        0,
        0,
        8,
        6,
        3,
        0,
        2,
        4,
        9,
        7,
        0,
    ], "if the semantic token list is [17, 84, 4, 2, 0], the error probably comes from the fact that the lexer is using an empty model"
